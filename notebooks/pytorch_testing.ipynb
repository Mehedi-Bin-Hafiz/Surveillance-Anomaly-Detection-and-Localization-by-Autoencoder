{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cb3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model loaded successfully from: ../models/model_e50_b4.h5\n",
      "\n",
      "Successfully converted Keras model to ONNX at: ../models/model_e50_b4.onnx\n",
      "ONNX model is valid.\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Custom loss function (from your original code)\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error loss between true and predicted values.\n",
    "    \"\"\"\n",
    "    mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    return mse_loss\n",
    "\n",
    "# Define the path to your Keras model and the output ONNX model\n",
    "keras_model_path = \"../models/model_e50_b4.h5\"\n",
    "onnx_model_path = \"../models/model_e50_b4.onnx\"\n",
    "\n",
    "# Ensure the Keras model file exists\n",
    "if not os.path.exists(keras_model_path):\n",
    "    print(f\"Error: Keras model file not found at {keras_model_path}\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the Keras model\n",
    "        keras_model = tf.keras.models.load_model(keras_model_path, custom_objects={'combined_loss': combined_loss})\n",
    "        print(f\"Keras model loaded successfully from: {keras_model_path}\")\n",
    "        \n",
    "        # Convert the Keras model to ONNX\n",
    "        model_proto, _ = tf2onnx.convert.from_keras(keras_model, output_path=onnx_model_path)\n",
    "        print(f\"\\nSuccessfully converted Keras model to ONNX at: {onnx_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ONNX conversion: {e}\")\n",
    "\n",
    "import onnx\n",
    "onnx_model_path = \"../models/model_e50_b4.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d221b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## guide to convert ONNX model to TensorRT engine\n",
    "## note: tersorRT must be installed and configured properly at first. \n",
    "# converting model.onnx to TensorRT engine\n",
    "# pip install tensorrt pycuda\n",
    "\n",
    "# cd models\n",
    "# trtexec --onnx=model_e50_b4.onnx --saveEngine=model_e50_b4.trt\n",
    "# trtexec --onnx=model_e50_b4.onnx --saveEngine=model_e50_b4.trt --fp16 --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29efa13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n",
      "Expected input shape for 'encoder_conv1_input': ['unk__129', 232, 232, 1]\n",
      "Using device: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Total frames in video: 200\n",
      "Frame 1 Loss (ONNX): 0.00275\n",
      "Frame 2 Loss (ONNX): 0.00274\n",
      "Frame 3 Loss (ONNX): 0.00278\n",
      "Frame 4 Loss (ONNX): 0.00272\n",
      "Frame 5 Loss (ONNX): 0.00272\n",
      "Frame 6 Loss (ONNX): 0.00278\n",
      "Frame 7 Loss (ONNX): 0.00280\n",
      "Frame 8 Loss (ONNX): 0.00282\n",
      "Frame 9 Loss (ONNX): 0.00287\n",
      "Frame 10 Loss (ONNX): 0.00281\n",
      "Frame 11 Loss (ONNX): 0.00278\n",
      "Frame 12 Loss (ONNX): 0.00273\n",
      "Frame 13 Loss (ONNX): 0.00287\n",
      "Frame 14 Loss (ONNX): 0.00271\n",
      "Frame 15 Loss (ONNX): 0.00267\n",
      "Frame 16 Loss (ONNX): 0.00268\n",
      "Frame 17 Loss (ONNX): 0.00273\n",
      "Frame 18 Loss (ONNX): 0.00272\n",
      "Frame 19 Loss (ONNX): 0.00271\n",
      "Frame 20 Loss (ONNX): 0.00279\n",
      "Frame 21 Loss (ONNX): 0.00272\n",
      "Frame 22 Loss (ONNX): 0.00265\n",
      "Frame 23 Loss (ONNX): 0.00270\n",
      "Frame 24 Loss (ONNX): 0.00269\n",
      "Frame 25 Loss (ONNX): 0.00274\n",
      "Frame 26 Loss (ONNX): 0.00262\n",
      "Frame 27 Loss (ONNX): 0.00257\n",
      "Frame 28 Loss (ONNX): 0.00257\n",
      "Frame 29 Loss (ONNX): 0.00256\n",
      "Frame 30 Loss (ONNX): 0.00256\n",
      "Frame 31 Loss (ONNX): 0.00255\n",
      "Frame 32 Loss (ONNX): 0.00253\n",
      "Frame 33 Loss (ONNX): 0.00258\n",
      "Frame 34 Loss (ONNX): 0.00259\n",
      "Frame 35 Loss (ONNX): 0.00256\n",
      "Frame 36 Loss (ONNX): 0.00259\n",
      "Frame 37 Loss (ONNX): 0.00263\n",
      "Frame 38 Loss (ONNX): 0.00262\n",
      "Frame 39 Loss (ONNX): 0.00258\n",
      "Frame 40 Loss (ONNX): 0.00258\n",
      "Frame 41 Loss (ONNX): 0.00262\n",
      "Frame 42 Loss (ONNX): 0.00272\n",
      "Frame 43 Loss (ONNX): 0.00280\n",
      "Frame 44 Loss (ONNX): 0.00285\n",
      "Frame 45 Loss (ONNX): 0.00284\n",
      "Frame 46 Loss (ONNX): 0.00282\n",
      "Frame 47 Loss (ONNX): 0.00288\n",
      "Frame 48 Loss (ONNX): 0.00280\n",
      "Frame 49 Loss (ONNX): 0.00304\n",
      "Frame 50 Loss (ONNX): 0.00300\n",
      "Frame 51 Loss (ONNX): 0.00288\n",
      "Frame 52 Loss (ONNX): 0.00285\n",
      "Frame 53 Loss (ONNX): 0.00302\n",
      "Frame 54 Loss (ONNX): 0.00302\n",
      "Frame 55 Loss (ONNX): 0.00305\n",
      "Frame 56 Loss (ONNX): 0.00314\n",
      "Frame 57 Loss (ONNX): 0.00300\n",
      "Frame 58 Loss (ONNX): 0.00315\n",
      "Frame 59 Loss (ONNX): 0.00405\n",
      "Frame 60 Loss (ONNX): 0.00375\n",
      "Frame 61 Loss (ONNX): 0.00335\n",
      "Frame 62 Loss (ONNX): 0.00317\n",
      "Frame 63 Loss (ONNX): 0.00326\n",
      "Frame 64 Loss (ONNX): 0.00354\n",
      "Frame 65 Loss (ONNX): 0.00478\n",
      "Frame 66 Loss (ONNX): 0.00471\n",
      "Frame 67 Loss (ONNX): 0.00415\n",
      "Frame 68 Loss (ONNX): 0.00368\n",
      "Frame 69 Loss (ONNX): 0.00353\n",
      "Frame 70 Loss (ONNX): 0.00379\n",
      "Frame 71 Loss (ONNX): 0.00449\n",
      "Frame 72 Loss (ONNX): 0.00392\n",
      "Frame 73 Loss (ONNX): 0.00396\n",
      "Frame 74 Loss (ONNX): 0.00497\n",
      "Frame 75 Loss (ONNX): 0.00695\n",
      "Frame 76 Loss (ONNX): 0.00708\n",
      "Frame 77 Loss (ONNX): 0.00652\n",
      "Frame 78 Loss (ONNX): 0.00613\n",
      "Frame 79 Loss (ONNX): 0.00496\n",
      "Frame 80 Loss (ONNX): 0.00544\n",
      "Frame 81 Loss (ONNX): 0.00594\n",
      "Frame 82 Loss (ONNX): 0.00569\n",
      "Frame 83 Loss (ONNX): 0.00598\n",
      "Frame 84 Loss (ONNX): 0.00631\n",
      "Frame 85 Loss (ONNX): 0.00643\n",
      "Frame 86 Loss (ONNX): 0.00717\n",
      "Frame 87 Loss (ONNX): 0.00667\n",
      "Frame 88 Loss (ONNX): 0.00598\n",
      "Frame 89 Loss (ONNX): 0.00579\n",
      "Frame 90 Loss (ONNX): 0.00588\n",
      "Frame 91 Loss (ONNX): 0.00592\n",
      "Frame 92 Loss (ONNX): 0.00705\n",
      "Frame 93 Loss (ONNX): 0.00654\n",
      "Frame 94 Loss (ONNX): 0.00750\n",
      "Frame 95 Loss (ONNX): 0.00916\n",
      "Frame 96 Loss (ONNX): 0.00980\n",
      "Frame 97 Loss (ONNX): 0.00899\n",
      "Frame 98 Loss (ONNX): 0.00787\n",
      "Frame 99 Loss (ONNX): 0.00688\n",
      "Frame 100 Loss (ONNX): 0.00667\n",
      "Processed 100 frames, found 30 anomalies so far (ONNX).\n",
      "Frame 101 Loss (ONNX): 0.00710\n",
      "Frame 102 Loss (ONNX): 0.00700\n",
      "Frame 103 Loss (ONNX): 0.00798\n",
      "Frame 104 Loss (ONNX): 0.00906\n",
      "Frame 105 Loss (ONNX): 0.00985\n",
      "Frame 106 Loss (ONNX): 0.01029\n",
      "Frame 107 Loss (ONNX): 0.00933\n",
      "Frame 108 Loss (ONNX): 0.00783\n",
      "Frame 109 Loss (ONNX): 0.00755\n",
      "Frame 110 Loss (ONNX): 0.00877\n",
      "Frame 111 Loss (ONNX): 0.00783\n",
      "Frame 112 Loss (ONNX): 0.00741\n",
      "Frame 113 Loss (ONNX): 0.00836\n",
      "Frame 114 Loss (ONNX): 0.01024\n",
      "Frame 115 Loss (ONNX): 0.01062\n",
      "Frame 116 Loss (ONNX): 0.01032\n",
      "Frame 117 Loss (ONNX): 0.00967\n",
      "Frame 118 Loss (ONNX): 0.00953\n",
      "Frame 119 Loss (ONNX): 0.00949\n",
      "Frame 120 Loss (ONNX): 0.00965\n",
      "Frame 121 Loss (ONNX): 0.01262\n",
      "Frame 122 Loss (ONNX): 0.01267\n",
      "Frame 123 Loss (ONNX): 0.01129\n",
      "Frame 124 Loss (ONNX): 0.01088\n",
      "Frame 125 Loss (ONNX): 0.01075\n",
      "Frame 126 Loss (ONNX): 0.01181\n",
      "Frame 127 Loss (ONNX): 0.01229\n",
      "Frame 128 Loss (ONNX): 0.01300\n",
      "Frame 129 Loss (ONNX): 0.00955\n",
      "Frame 130 Loss (ONNX): 0.00915\n",
      "Frame 131 Loss (ONNX): 0.01020\n",
      "Frame 132 Loss (ONNX): 0.01114\n",
      "Frame 133 Loss (ONNX): 0.01393\n",
      "Frame 134 Loss (ONNX): 0.01043\n",
      "Frame 135 Loss (ONNX): 0.01212\n",
      "Frame 136 Loss (ONNX): 0.01224\n",
      "Frame 137 Loss (ONNX): 0.01669\n",
      "Frame 138 Loss (ONNX): 0.01042\n",
      "Frame 139 Loss (ONNX): 0.01609\n",
      "Frame 140 Loss (ONNX): 0.01294\n",
      "Frame 141 Loss (ONNX): 0.01638\n",
      "Frame 142 Loss (ONNX): 0.01311\n",
      "Frame 143 Loss (ONNX): 0.01207\n",
      "Frame 144 Loss (ONNX): 0.01955\n",
      "Frame 145 Loss (ONNX): 0.01253\n",
      "Frame 146 Loss (ONNX): 0.01848\n",
      "Frame 147 Loss (ONNX): 0.01801\n",
      "Frame 148 Loss (ONNX): 0.01528\n",
      "Frame 149 Loss (ONNX): 0.01987\n",
      "Frame 150 Loss (ONNX): 0.02081\n",
      "Frame 151 Loss (ONNX): 0.01611\n",
      "Frame 152 Loss (ONNX): 0.02131\n",
      "Frame 153 Loss (ONNX): 0.02404\n",
      "Frame 154 Loss (ONNX): 0.02218\n",
      "Frame 155 Loss (ONNX): 0.02486\n",
      "Frame 156 Loss (ONNX): 0.02788\n",
      "Frame 157 Loss (ONNX): 0.02404\n",
      "Frame 158 Loss (ONNX): 0.02518\n",
      "Frame 159 Loss (ONNX): 0.01974\n",
      "Frame 160 Loss (ONNX): 0.02080\n",
      "Frame 161 Loss (ONNX): 0.01764\n",
      "Frame 162 Loss (ONNX): 0.01669\n",
      "Frame 163 Loss (ONNX): 0.01724\n",
      "Frame 164 Loss (ONNX): 0.01659\n",
      "Frame 165 Loss (ONNX): 0.01799\n",
      "Frame 166 Loss (ONNX): 0.01653\n",
      "Frame 167 Loss (ONNX): 0.01564\n",
      "Frame 168 Loss (ONNX): 0.01370\n",
      "Frame 169 Loss (ONNX): 0.01077\n",
      "Frame 170 Loss (ONNX): 0.00723\n",
      "Frame 171 Loss (ONNX): 0.00219\n",
      "Frame 172 Loss (ONNX): 0.00178\n",
      "Frame 173 Loss (ONNX): 0.00181\n",
      "Frame 174 Loss (ONNX): 0.00185\n",
      "Frame 175 Loss (ONNX): 0.00183\n",
      "Frame 176 Loss (ONNX): 0.00182\n",
      "Frame 177 Loss (ONNX): 0.00183\n",
      "Frame 178 Loss (ONNX): 0.00187\n",
      "Frame 179 Loss (ONNX): 0.00191\n",
      "Frame 180 Loss (ONNX): 0.00189\n",
      "Frame 181 Loss (ONNX): 0.00189\n",
      "Frame 182 Loss (ONNX): 0.00184\n",
      "Frame 183 Loss (ONNX): 0.00182\n",
      "Frame 184 Loss (ONNX): 0.00184\n",
      "Frame 185 Loss (ONNX): 0.00188\n",
      "Frame 186 Loss (ONNX): 0.00185\n",
      "Frame 187 Loss (ONNX): 0.00184\n",
      "Frame 188 Loss (ONNX): 0.00189\n",
      "Frame 189 Loss (ONNX): 0.00187\n",
      "Frame 190 Loss (ONNX): 0.00192\n",
      "Frame 191 Loss (ONNX): 0.00193\n",
      "Frame 192 Loss (ONNX): 0.00192\n",
      "Frame 193 Loss (ONNX): 0.00189\n",
      "Frame 194 Loss (ONNX): 0.00186\n",
      "Frame 195 Loss (ONNX): 0.00186\n",
      "Frame 196 Loss (ONNX): 0.00188\n",
      "Frame 197 Loss (ONNX): 0.00184\n",
      "Frame 198 Loss (ONNX): 0.00183\n",
      "Frame 199 Loss (ONNX): 0.00183\n",
      "Frame 200 Loss (ONNX): 0.00179\n",
      "Processed 200 frames, found 100 anomalies so far (ONNX).\n",
      "End of video or issue reading frame.\n",
      "\n",
      "Processing complete (ONNX)!\n",
      "Total frames processed: 200\n",
      "Total anomalies detected: 100\n",
      "Anomaly rate: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import os\n",
    "import onnxruntime\n",
    "import torch\n",
    "\n",
    "# Adjusted threshold for overall frame anomaly detection\n",
    "ANOMALY_THRESHOLD = 0.00435\n",
    "\n",
    "# Threshold for identifying anomalous regions within a frame\n",
    "ANOMALY_REGION_PIXEL_THRESHOLD = 0.200\n",
    "\n",
    "onnx_model_path = \"../models/model_e50_b4.onnx\"\n",
    "video_path = \"D:\\\\Research Dataset\\\\Surveillance\\\\Test\\\\testing_video.mp4\"\n",
    "\n",
    "# Ensure output folders exist\n",
    "if not os.path.exists(\"output_frames\"):\n",
    "    os.makedirs(\"output_frames\")\n",
    "if not os.path.exists(\"reconstructed_frames\"):\n",
    "    os.makedirs(\"reconstructed_frames\")\n",
    "if not os.path.exists(\"anomaly_maps\"):\n",
    "    os.makedirs(\"anomaly_maps\")\n",
    "\n",
    "frame_count_processed = 0\n",
    "anomaly_count = 0\n",
    "\n",
    "def mean_squared_loss(x1, x2):\n",
    "    \"\"\"\n",
    "    Calculates the true Mean Squared Error (MSE) between two arrays.\n",
    "    \"\"\"\n",
    "    x1_np = x1 if isinstance(x1, np.ndarray) else x1.cpu().numpy()\n",
    "    x2_np = x2 if isinstance(x2, np.ndarray) else x2.cpu().numpy()\n",
    "    difference = x1_np.flatten() - x2_np.flatten()\n",
    "    sq_difference = difference ** 2\n",
    "    mse = np.mean(sq_difference)\n",
    "    return mse\n",
    "\n",
    "def preprocess_frame_for_onnx_pytorch(frame):\n",
    "    \"\"\"\n",
    "    Preprocess frame for ONNX and returns a PyTorch tensor (NHWC).\n",
    "    \"\"\"\n",
    "    frame_resized = cv2.resize(frame, (232, 232), interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "    normalized_gray = (gray - 0.5) * 2.0\n",
    "    prep_frame = np.expand_dims(normalized_gray, axis=-1)\n",
    "    model_input_np = np.expand_dims(prep_frame, axis=0)\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    model_input_torch = torch.from_numpy(model_input_np).float()\n",
    "    return model_input_torch\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "try:\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_model_path, providers=providers)\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    output_name = ort_session.get_outputs()[0].name\n",
    "    print(\"ONNX model loaded successfully.\")\n",
    "    print(f\"Expected input shape for '{input_name}': {ort_session.get_inputs()[0].shape}\")\n",
    "\n",
    "    # Check the actual provider used\n",
    "    print(f\"Using device: {ort_session.get_providers()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ONNX model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Open the video file for processing\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file at {video_path}\")\n",
    "    exit()\n",
    "\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Total frames in video: {length}\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or issue reading frame.\")\n",
    "        break\n",
    "\n",
    "    frame_count_processed += 1\n",
    "\n",
    "    original_frame_color = imutils.resize(frame, width=700, height=600)\n",
    "\n",
    "    # Preprocess frame for ONNX and get a PyTorch tensor\n",
    "    processed_frame_torch = preprocess_frame_for_onnx_pytorch(frame)\n",
    "    processed_frame_onnx = processed_frame_torch.numpy() # Convert to NumPy for onnxruntime\n",
    "\n",
    "    # Run inference with the ONNX model\n",
    "    ort_inputs = {input_name: processed_frame_onnx}\n",
    "    ort_outputs = ort_session.run([output_name], ort_inputs)\n",
    "    reconstructed_frame_onnx = ort_outputs[0]\n",
    "    reconstructed_frame_torch = torch.from_numpy(reconstructed_frame_onnx).float() # Convert output to PyTorch tensor\n",
    "\n",
    "    # The output of the ONNX model is now a PyTorch tensor (reconstructed_frame_torch)\n",
    "    # and the input (as a tensor) was processed_frame_torch.\n",
    "\n",
    "    # Calculate the overall reconstruction loss (MSE) using NumPy for consistency\n",
    "    loss = mean_squared_loss(processed_frame_onnx, reconstructed_frame_onnx)\n",
    "    loss_display = f\"{loss:.5f}\"\n",
    "    print(f\"Frame {frame_count_processed} Loss (ONNX): {loss_display}\")\n",
    "\n",
    "    display_frame = original_frame_color.copy()\n",
    "    status_text = f\"Normal: {loss_display}\"\n",
    "    color = (0, 255, 0)\n",
    "\n",
    "    if loss > ANOMALY_THRESHOLD:\n",
    "        status_text = f\"Anomaly: {loss_display}\"\n",
    "        color = (0, 0, 255)\n",
    "        anomaly_count += 1\n",
    "\n",
    "        # --- Anomaly Localization (using ONNX output - still in NumPy for this part) ---\n",
    "        original_processed_np = processed_frame_onnx[0, :, :, 0]\n",
    "        reconstructed_frame_2d = reconstructed_frame_onnx[0, :, :, 0]\n",
    "        squared_diff_map = (original_processed_np - reconstructed_frame_2d)**2\n",
    "\n",
    "        error_map_display = cv2.normalize(squared_diff_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "        error_map_display_resized = cv2.resize(\n",
    "            error_map_display, (original_frame_color.shape[1], original_frame_color.shape[0]),\n",
    "            interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        cv2.imwrite(f\"anomaly_maps/frame_{frame_count_processed:05d}_error_map_onnx.jpg\", error_map_display_resized)\n",
    "\n",
    "        _, binary_error_mask = cv2.threshold(\n",
    "            (squared_diff_map * 255).astype(np.uint8),\n",
    "            (ANOMALY_REGION_PIXEL_THRESHOLD * 255),\n",
    "            255,\n",
    "            cv2.THRESH_BINARY\n",
    "        )\n",
    "        binary_error_mask_resized = cv2.resize(\n",
    "            binary_error_mask, (original_frame_color.shape[1], original_frame_color.shape[0]),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "\n",
    "        contours, _ = cv2.findContours(\n",
    "            binary_error_mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) > 50:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                overlay = display_frame.copy()\n",
    "                cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 0, 255), -1)\n",
    "                alpha = 0.50\n",
    "                display_frame = cv2.addWeighted(overlay, alpha, display_frame, 1 - alpha, 0)\n",
    "        # --- End Anomaly Localization ---\n",
    "\n",
    "    cv2.putText(display_frame, status_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    if status_text.startswith(\"Anomaly\"):\n",
    "        cv2.imwrite(f\"output_frames/frame_{frame_count_processed:05d}.jpg\", display_frame)\n",
    "    else:\n",
    "        cv2.imwrite(f\"output_frames/frame_{frame_count_processed:05d}.jpg\", display_frame)\n",
    "\n",
    "    cv2.imshow(\"Anomaly Detection (ONNX)\", display_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    if frame_count_processed % 100 == 0:\n",
    "        print(f\"Processed {frame_count_processed} frames, found {anomaly_count} anomalies so far (ONNX).\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"\\nProcessing complete (ONNX)!\")\n",
    "print(f\"Total frames processed: {frame_count_processed}\")\n",
    "print(f\"Total anomalies detected: {anomaly_count}\")\n",
    "if frame_count_processed > 0:\n",
    "    anomaly_rate = (anomaly_count / frame_count_processed * 100)\n",
    "    print(f\"Anomaly rate: {anomaly_rate:.2f}%\")\n",
    "else:\n",
    "    print(\"No frames were processed, cannot calculate anomaly rate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchVenv",
   "language": "python",
   "name": "torchvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
